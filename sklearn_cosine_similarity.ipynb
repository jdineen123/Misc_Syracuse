{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: How to Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Use numpy's dot and norm functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T02:32:34.777183Z",
     "start_time": "2018-08-28T02:32:34.772196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9486832980505138\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "X = [1,2]\n",
    "Y = [2,2]\n",
    "cos_sim = dot(X,Y) / (norm(X)*norm(Y))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Use scipy's built-in cosine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T02:32:36.598313Z",
     "start_time": "2018-08-28T02:32:36.179433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9486832980505138\n"
     ]
    }
   ],
   "source": [
    "# note that this function actually calculates cosine similarity \n",
    "# and then use \"1-similarity\" to convert similarity to distance\n",
    "# to get the actual cosine similarity, you need to do 1-distance\n",
    "\n",
    "from scipy import spatial\n",
    "X = [1,2]\n",
    "Y = [2,2]\n",
    "cos_sim = 1 - spatial.distance.cosine(X, Y)\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Use sklearn to calculate the cosine similarity matrix among vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T02:32:38.753548Z",
     "start_time": "2018-08-28T02:32:37.704354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9486833 1.       ]]\n",
      "[[1.        0.9486833 1.       ]\n",
      " [0.9486833 1.        0.9486833]\n",
      " [1.        0.9486833 1.       ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "X = np.array([1,2])\n",
    "Y = np.array([2,2])\n",
    "Z = np.array([2,4])\n",
    "\n",
    "# calculate cosine similarity between [X] and [Y,Z]\n",
    "# sending input as arrays would allow for calculating both cosine_sim(X,Y) and cosine_sim (X,Y)\n",
    "cos_sim = cosine_similarity([X], [Y,Z])\n",
    "print(cos_sim)\n",
    "\n",
    "# calculate the entire cosie similarity matrix among X, Y, and Z\n",
    "cos_sim = cosine_similarity([X, Y, Z])\n",
    "print(cos_sim)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cosine similarity for plagiarism detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T02:32:40.042102Z",
     "start_time": "2018-08-28T02:32:39.995228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 70)\n",
      "[[1.         0.69376514 0.21550898]]\n",
      "(1, 3)\n",
      "Contrast the condition into which all these friendly Indians are suddenly plunged now, with their condition only two years previous: martial law now in force on all their reservations; themselves in danger of starvation, and constantly exposed to the influence of emissaries from their friends and relations, urging them to join in fighting this treacherous government that had kept faith with nobody--neither with friend nor with foe.\n",
      "\n",
      "Only two years later, all these friendly Sioux were suddenly plunged into new conditions, including starvation, martial law on all their reservations, and constant urging by their friends and relations to join in warfare against the treacherous government that had kept faith with neither friend nor foe.\n"
     ]
    }
   ],
   "source": [
    "# data from https://www.bowdoin.edu/studentaffairs/academic-honesty/examples/mosaic/index.shtml\n",
    "\n",
    "txt_original = \"Contrast the condition into which all these friendly Indians are suddenly plunged now, with their condition only two years previous: martial law now in force on all their reservations; themselves in danger of starvation, and constantly exposed to the influence of emissaries from their friends and relations, urging them to join in fighting this treacherous government that had kept faith with nobody--neither with friend nor with foe.\"\n",
    "txt_plagiarized = \"Only two years later, all these friendly Sioux were suddenly plunged into new conditions, including starvation, martial law on all their reservations, and constant urging by their friends and relations to join in warfare against the treacherous government that had kept faith with neither friend nor foe.\"\n",
    "txt_control = \"Only two years later, all the money he won from lottery was gone.\"\n",
    "\n",
    "txts = [txt_original, txt_plagiarized, txt_control]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "unigram_count = CountVectorizer(encoding='latin-1', binary=False)\n",
    "unigram_count_stop_remove = CountVectorizer(encoding='latin-1', binary=True, stop_words='english')\n",
    "\n",
    "vecs = unigram_count.fit_transform(txts)\n",
    "\n",
    "print(vecs.shape)\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cos_sim = cosine_similarity(vecs[0], vecs)\n",
    "print(cos_sim)\n",
    "sim_sorted_doc_idx = cos_sim.argsort()\n",
    "print(sim_sorted_doc_idx.shape)\n",
    "\n",
    "# print the most similar doc; it's actually the original doc itself\n",
    "print(txts[sim_sorted_doc_idx[0][len(txts)-1]])\n",
    "print()\n",
    "\n",
    "# print the second most similar doc; it's the most likely plagiarized one\n",
    "print(txts[sim_sorted_doc_idx[0][len(txts)-2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T02:47:12.338320Z",
     "start_time": "2018-08-28T02:47:12.317377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos Similarity Scores with Boolean Vectorization:\n",
      "Doc 1: [[1.         0.81649658 0.81649658]] -- Most similar, Ranked: [[1 2 0]]\n",
      "Doc 2: [[0.81649658 1.         0.5       ]] -- Most similar, Ranked: [[2 0 1]]\n",
      "Doc 3: [[0.81649658 0.5        1.        ]] -- Most similar, Ranked: [[1 0 2]]\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "Cos Similarity Scores with Count Vectorization:\n",
      "Doc 1: [[1.         0.59628479 0.89442719]] -- Most similar, Ranked: [[1 2 0]]\n",
      "Doc 2: [[0.59628479 1.         0.2       ]] -- Most similar, Ranked: [[2 0 1]]\n",
      "Doc 3: [[0.89442719 0.2        1.        ]] -- Most similar, Ranked: [[1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "Doc1 = (\"book, book, music, video, video\")\n",
    "Doc2= (\"music, music, video\")\n",
    "Doc3= (\"book, book, video\")\n",
    "txts = [Doc1,Doc2,Doc3]\n",
    "\n",
    "boolean_unigram = CountVectorizer(encoding='latin-1', binary=True, stop_words='english', lowercase=False)\n",
    "count_unigram = CountVectorizer(encoding='latin-1', binary=False, stop_words='english', lowercase=False)\n",
    "vectorizers = [boolean_unigram, CountVectorizer]\n",
    "\n",
    "vec = boolean_unigram.fit_transform(txts)\n",
    "vec1 = count_unigram.fit_transform(txts)\n",
    " \n",
    "\n",
    "print('Cos Similarity Scores with Boolean Vectorization:')\n",
    "for i in range(0, 3):        \n",
    "    print('Doc {}: {} -- Most similar, Ranked: {}'.format(i +1 , cosine_similarity(vec[i], vec), cosine_similarity(vec[i], vec).argsort()))\n",
    "print('---------------------------------------------------')\n",
    "print('---------------------------------------------------')\n",
    "print('Cos Similarity Scores with Count Vectorization:')\n",
    "for i in range(0, 3):        \n",
    "    print('Doc {}: {} -- Most similar, Ranked: {}'.format(i +1 , cosine_similarity(vec1[i], vec1), cosine_similarity(vec1[i], vec1).argsort()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T04:51:09.925557Z",
     "start_time": "2018-08-28T04:51:09.525627Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "import sklearn\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "\n",
    "newsgroup = fetch_20newsgroups('all', categories= ['talk.politics.misc', 'talk.religion.misc'])\n",
    "count_unigram = CountVectorizer(encoding='latin-1', binary=False, stop_words='english', lowercase=False)\n",
    "\n",
    "\n",
    "X =newsgroup['data']\n",
    "y = newsgroup['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = unigram_count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T04:51:19.761756Z",
     "start_time": "2018-08-28T04:51:19.749788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the LinearSVC module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf = LinearSVC(C=1)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T04:57:43.927988Z",
     "start_time": "2018-08-28T04:57:43.916020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(-0.22775602422547228, 'sexual')\n",
      "(-0.16481286336424758, 'clinton')\n",
      "(-0.14921268049946385, 'york')\n",
      "(-0.14387612158057855, 'judge')\n",
      "(-0.1260833516960672, 'kaldis')\n",
      "(-0.12191641377190139, 'american')\n",
      "(-0.12170341046292382, 'open')\n",
      "(-0.1129558060598557, 'malcolm')\n",
      "(-0.10786750331199461, 'lines')\n",
      "(-0.10665460040984912, 'brandeis')\n",
      "\n",
      "positive words\n",
      "(0.1274565597259194, '15')\n",
      "(0.12985612675857158, 'suggestion')\n",
      "(0.13179645640026524, '666')\n",
      "(0.13289886219082053, 'promise')\n",
      "(0.134754092309615, 'buffalo')\n",
      "(0.1377677974968512, 'happened')\n",
      "(0.1386329374671371, 'info')\n",
      "(0.14200448134572397, 'morality')\n",
      "(0.18320065011216924, 'age')\n",
      "(0.22244676841279454, 'christian')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "\n",
    "very_negative_10 = feature_ranks[:10]\n",
    "\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = feature_ranks[-10:]\n",
    "print(\"positive words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T04:56:52.794736Z",
     "start_time": "2018-08-28T04:56:52.790747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3223)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(svm_clf.coef_)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
